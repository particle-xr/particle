---
title: Particle Space
---
Can we foresee a new "coordinate system" emerging? One in which we are not working in, e.g., Euclidean space but in some alternate spacial mechanism. Consider the idea that, in the particle universe, *everything* is modelled at the lowest "particle level". Does this mean that, simply by choosing such an architecture, we can form "space" based on particle arrangement?

We can envision the arrangement of our world as simply a bunch of nodes, which collectively make up the entire world. So the root node would be the world and every node that exists *within* the world would descend from it. Right away we can see that we will want whatever this data structure is to have all sorts of ways (i.e.e interfaces) which can be used to traverse (recursively) this complex tree structure. So, if we say that the [[genesis]] defines the *shape* of the world (i.e. where do nodes exist within the world space) then rendering will be a process wherein we recursively traverse the tree and apply transformations at various levels.

What if we were to have *two* (at least) of these trees? One would be on the server (which does the bulk of the rendering) and another would be on the client (i.e. the user's machine) which is constantly changing based on the local camera? This would give us the best of both worlds (seemingly). We can render some portion of the world on the big beefy server. On the client we basically just have to keep track of "what can we actually see". We could say that based on some client-side configuration, we have the concepts of "near" and "far". Hmm... this is leading to a seemingly complex "overlapping tree" algorithm to determine which nodes are in the user's field of vision. Maybe the crux is that the user somehow exists *within* this tree?
